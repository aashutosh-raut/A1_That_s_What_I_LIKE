{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use the NLTK Brown corpus as a real-world dataset.\n",
    "# The corpus is tokenized into sentences and words, and all tokens\n",
    "# are converted to lowercase to reduce vocabulary size.\n",
    "# This preprocessing step is important to ensure consistent embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import time\n",
    "import nltk.corpus \n",
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.0.2', '2.8.0+cpu', '3.9.4')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__, matplotlib.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Brown corpus from NLTK as a real-world text dataset.\n",
    "# This corpus contains texts from multiple genres, which helps\n",
    "# the model learn more diverse word contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\aashu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aashu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load Reuters corpus (real-world data for final training)\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "sentences = brown.sents(categories=\"news\")\n",
    "sentences = [[word.lower() for word in sent] for sent in sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We use the Brown news dataset provided by the NLTK library.  \n",
    "This dataset contains news articles across multiple topics and is commonly used for NLP research.\n",
    "Using this dataset allows us to train word embeddings on real-world text instead of toy examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized sentences: 4623\n",
      "Sample tokenized sentence (first 30 words):\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize Reuters corpus\n",
    "\n",
    "tokenized_sentences = sentences\n",
    "\n",
    "print(\"Number of tokenized sentences:\", len(tokenized_sentences))\n",
    "print(\"Sample tokenized sentence (first 30 words):\")\n",
    "print(tokenized_sentences[0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate (target, context) word pairs for Skip-gram training.\n",
    "# For each target word, surrounding context words are selected\n",
    "# using a dynamic window size (default = 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (center, context) pairs generated: 374548\n",
      "Sample (center, context) pairs: [('The', 'Fulton'), ('The', 'County'), ('Fulton', 'The'), ('Fulton', 'County'), ('Fulton', 'Grand'), ('County', 'The'), ('County', 'Fulton'), ('County', 'Grand'), ('County', 'Jury'), ('Grand', 'Fulton')]\n"
     ]
    }
   ],
   "source": [
    "def generate_context(sentence, center_idx, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate context words for a given center word index using a dynamic window size.\n",
    "    \"\"\"\n",
    "    start = max(0, center_idx - window_size)\n",
    "    end = min(len(sentence), center_idx + window_size + 1)\n",
    "    return [sentence[j] for j in range(start, end) if j != center_idx]\n",
    "\n",
    "\n",
    "def generate_skipgram_pairs(sentences, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate (center, context) pairs for Skip-gram training.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for sentence in sentences:\n",
    "        for i, center in enumerate(sentence):\n",
    "            context_words = generate_context(sentence, i, window_size)\n",
    "            for context in context_words:\n",
    "                pairs.append((center, context))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Default window size\n",
    "window_size = 2\n",
    "\n",
    "# Generate Skip-gram training pairs\n",
    "pairs = generate_skipgram_pairs(tokenized_sentences, window_size)\n",
    "print(\"Number of (center, context) pairs generated:\", len(pairs))\n",
    "print(\"Sample (center, context) pairs:\", pairs[:10])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, sentences, word2index, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate a random batch of (center, context) word index pairs\n",
    "    using a dynamic window size.\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    while len(inputs) < batch_size:\n",
    "        # Randomly choose a sentence\n",
    "        sentence = sentences[np.random.randint(len(sentences))]\n",
    "        \n",
    "        # Randomly choose a center word index\n",
    "        center_idx = np.random.randint(len(sentence))\n",
    "        center_word = sentence[center_idx]\n",
    "        center = word2index.get(center_word, word2index[\"<UNK>\"])\n",
    "\n",
    "        # Generate context words dynamically\n",
    "        context_words = generate_context(sentence, center_idx, window_size)\n",
    "\n",
    "        for context_word in context_words:\n",
    "            if len(inputs) >= batch_size:\n",
    "                break\n",
    "            context = word2index.get(context_word, word2index[\"<UNK>\"])\n",
    "            inputs.append(center)\n",
    "            labels.append(context)\n",
    "\n",
    "    return np.array(inputs), np.array(labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.embedding_center = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "\n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        \"\"\"\n",
    "        center: (batch_size, 1)\n",
    "        outside: (batch_size, 1)\n",
    "        all_vocabs: (batch_size, voc_size)\n",
    "        \"\"\"\n",
    "        v_c = self.embedding_center(center)          # (B, 1, D)\n",
    "        u_o = self.embedding_outside(outside)        # (B, 1, D)\n",
    "        u_all = self.embedding_outside(all_vocabs)   # (B, V, D)\n",
    "\n",
    "        # Positive score\n",
    "        score = torch.bmm(u_o, v_c.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        # All vocabulary scores\n",
    "        all_scores = torch.bmm(u_all, v_c.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        log_probs = torch.log_softmax(all_scores, dim=1)\n",
    "\n",
    "        loss = -torch.mean(log_probs.gather(1, outside))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "for doc in tokenized_sentences:\n",
    "    unique_words.update(doc)\n",
    "\n",
    "vocabs = list(unique_words)\n",
    "vocabs.append(\"<UNK>\")\n",
    "\n",
    "word2index = {word: idx for idx, word in enumerate(vocabs)}\n",
    "tokenized_corpus = tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(vocabs)\n",
    "emb_size = 8   # recommended: 50 (fast) or 100 (better)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_vocabs(vocabs, word2index, batch_size):\n",
    "    \"\"\"\n",
    "    Prepare a tensor containing all vocabulary indices,\n",
    "    expanded for batch-wise full softmax computation.\n",
    "    \"\"\"\n",
    "    idxs = [word2index.get(w, word2index[\"<UNK>\"]) for w in vocabs]\n",
    "    all_vocab_tensor = torch.LongTensor(idxs).unsqueeze(0)\n",
    "    return all_vocab_tensor.expand(batch_size, len(vocabs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Skip-gram model over multiple epochs.\n",
    "# The training loss is tracked to monitor convergence\n",
    "# and evaluate how well the embeddings are learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Skipgram(voc_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "window_size = 2  # DEFAULT (assignment requirement)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - Loss: 7.6487\n",
      "Epoch [2/1000] - Loss: 7.4810\n",
      "Epoch [3/1000] - Loss: 7.6563\n",
      "Epoch [4/1000] - Loss: 7.4523\n",
      "Epoch [5/1000] - Loss: 7.4586\n",
      "Epoch [6/1000] - Loss: 7.6114\n",
      "Epoch [7/1000] - Loss: 7.4958\n",
      "Epoch [8/1000] - Loss: 7.5064\n",
      "Epoch [9/1000] - Loss: 7.6241\n",
      "Epoch [10/1000] - Loss: 7.7356\n",
      "Epoch [11/1000] - Loss: 7.5463\n",
      "Epoch [12/1000] - Loss: 7.4844\n",
      "Epoch [13/1000] - Loss: 7.3391\n",
      "Epoch [14/1000] - Loss: 7.4598\n",
      "Epoch [15/1000] - Loss: 7.3833\n",
      "Epoch [16/1000] - Loss: 7.4574\n",
      "Epoch [17/1000] - Loss: 7.6435\n",
      "Epoch [18/1000] - Loss: 7.4926\n",
      "Epoch [19/1000] - Loss: 7.5898\n",
      "Epoch [20/1000] - Loss: 7.3970\n",
      "Epoch [21/1000] - Loss: 7.3083\n",
      "Epoch [22/1000] - Loss: 7.6136\n",
      "Epoch [23/1000] - Loss: 7.4591\n",
      "Epoch [24/1000] - Loss: 7.6034\n",
      "Epoch [25/1000] - Loss: 7.5899\n",
      "Epoch [26/1000] - Loss: 7.2284\n",
      "Epoch [27/1000] - Loss: 7.3611\n",
      "Epoch [28/1000] - Loss: 7.6273\n",
      "Epoch [29/1000] - Loss: 7.5318\n",
      "Epoch [30/1000] - Loss: 7.4335\n",
      "Epoch [31/1000] - Loss: 7.4373\n",
      "Epoch [32/1000] - Loss: 7.2315\n",
      "Epoch [33/1000] - Loss: 7.6953\n",
      "Epoch [34/1000] - Loss: 7.4727\n",
      "Epoch [35/1000] - Loss: 7.5432\n",
      "Epoch [36/1000] - Loss: 7.7932\n",
      "Epoch [37/1000] - Loss: 7.3889\n",
      "Epoch [38/1000] - Loss: 7.5850\n",
      "Epoch [39/1000] - Loss: 7.6316\n",
      "Epoch [40/1000] - Loss: 7.4570\n",
      "Epoch [41/1000] - Loss: 7.4764\n",
      "Epoch [42/1000] - Loss: 7.7477\n",
      "Epoch [43/1000] - Loss: 7.6893\n",
      "Epoch [44/1000] - Loss: 7.7825\n",
      "Epoch [45/1000] - Loss: 7.6399\n",
      "Epoch [46/1000] - Loss: 7.5601\n",
      "Epoch [47/1000] - Loss: 7.3036\n",
      "Epoch [48/1000] - Loss: 7.5186\n",
      "Epoch [49/1000] - Loss: 7.3818\n",
      "Epoch [50/1000] - Loss: 7.4851\n",
      "Epoch [51/1000] - Loss: 7.6188\n",
      "Epoch [52/1000] - Loss: 7.8212\n",
      "Epoch [53/1000] - Loss: 7.4059\n",
      "Epoch [54/1000] - Loss: 7.6694\n",
      "Epoch [55/1000] - Loss: 7.4759\n",
      "Epoch [56/1000] - Loss: 7.3839\n",
      "Epoch [57/1000] - Loss: 7.5651\n",
      "Epoch [58/1000] - Loss: 7.6212\n",
      "Epoch [59/1000] - Loss: 7.3607\n",
      "Epoch [60/1000] - Loss: 7.3607\n",
      "Epoch [61/1000] - Loss: 7.2693\n",
      "Epoch [62/1000] - Loss: 7.5140\n",
      "Epoch [63/1000] - Loss: 7.3953\n",
      "Epoch [64/1000] - Loss: 7.5754\n",
      "Epoch [65/1000] - Loss: 7.3214\n",
      "Epoch [66/1000] - Loss: 7.4122\n",
      "Epoch [67/1000] - Loss: 7.4451\n",
      "Epoch [68/1000] - Loss: 7.3383\n",
      "Epoch [69/1000] - Loss: 7.6593\n",
      "Epoch [70/1000] - Loss: 7.5422\n",
      "Epoch [71/1000] - Loss: 7.3271\n",
      "Epoch [72/1000] - Loss: 7.7636\n",
      "Epoch [73/1000] - Loss: 7.6053\n",
      "Epoch [74/1000] - Loss: 7.6660\n",
      "Epoch [75/1000] - Loss: 7.4895\n",
      "Epoch [76/1000] - Loss: 7.8076\n",
      "Epoch [77/1000] - Loss: 7.4660\n",
      "Epoch [78/1000] - Loss: 7.3123\n",
      "Epoch [79/1000] - Loss: 7.4191\n",
      "Epoch [80/1000] - Loss: 7.5351\n",
      "Epoch [81/1000] - Loss: 7.4137\n",
      "Epoch [82/1000] - Loss: 7.7614\n",
      "Epoch [83/1000] - Loss: 7.4408\n",
      "Epoch [84/1000] - Loss: 7.6832\n",
      "Epoch [85/1000] - Loss: 7.4114\n",
      "Epoch [86/1000] - Loss: 7.6037\n",
      "Epoch [87/1000] - Loss: 7.6433\n",
      "Epoch [88/1000] - Loss: 7.4592\n",
      "Epoch [89/1000] - Loss: 7.5210\n",
      "Epoch [90/1000] - Loss: 7.5635\n",
      "Epoch [91/1000] - Loss: 7.4731\n",
      "Epoch [92/1000] - Loss: 7.5133\n",
      "Epoch [93/1000] - Loss: 7.5445\n",
      "Epoch [94/1000] - Loss: 7.4274\n",
      "Epoch [95/1000] - Loss: 7.5055\n",
      "Epoch [96/1000] - Loss: 7.4861\n",
      "Epoch [97/1000] - Loss: 7.3610\n",
      "Epoch [98/1000] - Loss: 7.5016\n",
      "Epoch [99/1000] - Loss: 7.4133\n",
      "Epoch [100/1000] - Loss: 7.3902\n",
      "Epoch [101/1000] - Loss: 7.4977\n",
      "Epoch [102/1000] - Loss: 7.2675\n",
      "Epoch [103/1000] - Loss: 7.5050\n",
      "Epoch [104/1000] - Loss: 7.2106\n",
      "Epoch [105/1000] - Loss: 7.2802\n",
      "Epoch [106/1000] - Loss: 7.3558\n",
      "Epoch [107/1000] - Loss: 7.8610\n",
      "Epoch [108/1000] - Loss: 7.2653\n",
      "Epoch [109/1000] - Loss: 7.4970\n",
      "Epoch [110/1000] - Loss: 7.3035\n",
      "Epoch [111/1000] - Loss: 7.4689\n",
      "Epoch [112/1000] - Loss: 7.6415\n",
      "Epoch [113/1000] - Loss: 7.3856\n",
      "Epoch [114/1000] - Loss: 7.4266\n",
      "Epoch [115/1000] - Loss: 7.2675\n",
      "Epoch [116/1000] - Loss: 7.4277\n",
      "Epoch [117/1000] - Loss: 7.4139\n",
      "Epoch [118/1000] - Loss: 7.2714\n",
      "Epoch [119/1000] - Loss: 7.4220\n",
      "Epoch [120/1000] - Loss: 7.2904\n",
      "Epoch [121/1000] - Loss: 7.3823\n",
      "Epoch [122/1000] - Loss: 7.4839\n",
      "Epoch [123/1000] - Loss: 7.5571\n",
      "Epoch [124/1000] - Loss: 7.6879\n",
      "Epoch [125/1000] - Loss: 7.4959\n",
      "Epoch [126/1000] - Loss: 7.4159\n",
      "Epoch [127/1000] - Loss: 7.6017\n",
      "Epoch [128/1000] - Loss: 7.5950\n",
      "Epoch [129/1000] - Loss: 7.4718\n",
      "Epoch [130/1000] - Loss: 7.9010\n",
      "Epoch [131/1000] - Loss: 7.5160\n",
      "Epoch [132/1000] - Loss: 7.4027\n",
      "Epoch [133/1000] - Loss: 7.5128\n",
      "Epoch [134/1000] - Loss: 7.4187\n",
      "Epoch [135/1000] - Loss: 7.4822\n",
      "Epoch [136/1000] - Loss: 7.4058\n",
      "Epoch [137/1000] - Loss: 7.7245\n",
      "Epoch [138/1000] - Loss: 7.4289\n",
      "Epoch [139/1000] - Loss: 7.3877\n",
      "Epoch [140/1000] - Loss: 7.5772\n",
      "Epoch [141/1000] - Loss: 7.4799\n",
      "Epoch [142/1000] - Loss: 7.5139\n",
      "Epoch [143/1000] - Loss: 7.5820\n",
      "Epoch [144/1000] - Loss: 7.4645\n",
      "Epoch [145/1000] - Loss: 7.3956\n",
      "Epoch [146/1000] - Loss: 7.4359\n",
      "Epoch [147/1000] - Loss: 7.4585\n",
      "Epoch [148/1000] - Loss: 7.4024\n",
      "Epoch [149/1000] - Loss: 7.3489\n",
      "Epoch [150/1000] - Loss: 7.6582\n",
      "Epoch [151/1000] - Loss: 7.9188\n",
      "Epoch [152/1000] - Loss: 7.7573\n",
      "Epoch [153/1000] - Loss: 7.2905\n",
      "Epoch [154/1000] - Loss: 7.5018\n",
      "Epoch [155/1000] - Loss: 7.3691\n",
      "Epoch [156/1000] - Loss: 7.3392\n",
      "Epoch [157/1000] - Loss: 7.3286\n",
      "Epoch [158/1000] - Loss: 7.6633\n",
      "Epoch [159/1000] - Loss: 7.4770\n",
      "Epoch [160/1000] - Loss: 7.4799\n",
      "Epoch [161/1000] - Loss: 7.5524\n",
      "Epoch [162/1000] - Loss: 7.4633\n",
      "Epoch [163/1000] - Loss: 7.3989\n",
      "Epoch [164/1000] - Loss: 7.4979\n",
      "Epoch [165/1000] - Loss: 7.3739\n",
      "Epoch [166/1000] - Loss: 7.6022\n",
      "Epoch [167/1000] - Loss: 7.5368\n",
      "Epoch [168/1000] - Loss: 7.6703\n",
      "Epoch [169/1000] - Loss: 7.3240\n",
      "Epoch [170/1000] - Loss: 7.3760\n",
      "Epoch [171/1000] - Loss: 7.3658\n",
      "Epoch [172/1000] - Loss: 7.3256\n",
      "Epoch [173/1000] - Loss: 7.5189\n",
      "Epoch [174/1000] - Loss: 7.3039\n",
      "Epoch [175/1000] - Loss: 7.2365\n",
      "Epoch [176/1000] - Loss: 7.1158\n",
      "Epoch [177/1000] - Loss: 7.3768\n",
      "Epoch [178/1000] - Loss: 7.5437\n",
      "Epoch [179/1000] - Loss: 7.5849\n",
      "Epoch [180/1000] - Loss: 7.6189\n",
      "Epoch [181/1000] - Loss: 7.3811\n",
      "Epoch [182/1000] - Loss: 7.3742\n",
      "Epoch [183/1000] - Loss: 7.5293\n",
      "Epoch [184/1000] - Loss: 7.2465\n",
      "Epoch [185/1000] - Loss: 7.2998\n",
      "Epoch [186/1000] - Loss: 7.4020\n",
      "Epoch [187/1000] - Loss: 7.7153\n",
      "Epoch [188/1000] - Loss: 7.3710\n",
      "Epoch [189/1000] - Loss: 7.8497\n",
      "Epoch [190/1000] - Loss: 7.2279\n",
      "Epoch [191/1000] - Loss: 7.4327\n",
      "Epoch [192/1000] - Loss: 7.7786\n",
      "Epoch [193/1000] - Loss: 7.3572\n",
      "Epoch [194/1000] - Loss: 7.5128\n",
      "Epoch [195/1000] - Loss: 7.3781\n",
      "Epoch [196/1000] - Loss: 7.4403\n",
      "Epoch [197/1000] - Loss: 7.3362\n",
      "Epoch [198/1000] - Loss: 7.3717\n",
      "Epoch [199/1000] - Loss: 7.2903\n",
      "Epoch [200/1000] - Loss: 7.2958\n",
      "Epoch [201/1000] - Loss: 7.5892\n",
      "Epoch [202/1000] - Loss: 7.5679\n",
      "Epoch [203/1000] - Loss: 7.8869\n",
      "Epoch [204/1000] - Loss: 7.5455\n",
      "Epoch [205/1000] - Loss: 7.6515\n",
      "Epoch [206/1000] - Loss: 7.4977\n",
      "Epoch [207/1000] - Loss: 7.7420\n",
      "Epoch [208/1000] - Loss: 7.4929\n",
      "Epoch [209/1000] - Loss: 7.0636\n",
      "Epoch [210/1000] - Loss: 7.2106\n",
      "Epoch [211/1000] - Loss: 7.6461\n",
      "Epoch [212/1000] - Loss: 7.5547\n",
      "Epoch [213/1000] - Loss: 7.6025\n",
      "Epoch [214/1000] - Loss: 7.1551\n",
      "Epoch [215/1000] - Loss: 7.4031\n",
      "Epoch [216/1000] - Loss: 7.5067\n",
      "Epoch [217/1000] - Loss: 7.5334\n",
      "Epoch [218/1000] - Loss: 7.4627\n",
      "Epoch [219/1000] - Loss: 7.3628\n",
      "Epoch [220/1000] - Loss: 7.2405\n",
      "Epoch [221/1000] - Loss: 7.4418\n",
      "Epoch [222/1000] - Loss: 7.3577\n",
      "Epoch [223/1000] - Loss: 7.2984\n",
      "Epoch [224/1000] - Loss: 7.3789\n",
      "Epoch [225/1000] - Loss: 7.5352\n",
      "Epoch [226/1000] - Loss: 7.5130\n",
      "Epoch [227/1000] - Loss: 7.5584\n",
      "Epoch [228/1000] - Loss: 7.4509\n",
      "Epoch [229/1000] - Loss: 7.5389\n",
      "Epoch [230/1000] - Loss: 7.5573\n",
      "Epoch [231/1000] - Loss: 7.3662\n",
      "Epoch [232/1000] - Loss: 7.5006\n",
      "Epoch [233/1000] - Loss: 7.4856\n",
      "Epoch [234/1000] - Loss: 7.4978\n",
      "Epoch [235/1000] - Loss: 7.6410\n",
      "Epoch [236/1000] - Loss: 7.7321\n",
      "Epoch [237/1000] - Loss: 7.5536\n",
      "Epoch [238/1000] - Loss: 7.3624\n",
      "Epoch [239/1000] - Loss: 7.6113\n",
      "Epoch [240/1000] - Loss: 7.3675\n",
      "Epoch [241/1000] - Loss: 7.4836\n",
      "Epoch [242/1000] - Loss: 7.1928\n",
      "Epoch [243/1000] - Loss: 7.6427\n",
      "Epoch [244/1000] - Loss: 7.3455\n",
      "Epoch [245/1000] - Loss: 7.3554\n",
      "Epoch [246/1000] - Loss: 7.4023\n",
      "Epoch [247/1000] - Loss: 7.5068\n",
      "Epoch [248/1000] - Loss: 7.4044\n",
      "Epoch [249/1000] - Loss: 7.4863\n",
      "Epoch [250/1000] - Loss: 7.5407\n",
      "Epoch [251/1000] - Loss: 7.3257\n",
      "Epoch [252/1000] - Loss: 7.4499\n",
      "Epoch [253/1000] - Loss: 7.4093\n",
      "Epoch [254/1000] - Loss: 7.2355\n",
      "Epoch [255/1000] - Loss: 7.3762\n",
      "Epoch [256/1000] - Loss: 7.7270\n",
      "Epoch [257/1000] - Loss: 7.5517\n",
      "Epoch [258/1000] - Loss: 7.5039\n",
      "Epoch [259/1000] - Loss: 7.5259\n",
      "Epoch [260/1000] - Loss: 7.4649\n",
      "Epoch [261/1000] - Loss: 7.5686\n",
      "Epoch [262/1000] - Loss: 7.4581\n",
      "Epoch [263/1000] - Loss: 7.2978\n",
      "Epoch [264/1000] - Loss: 7.8470\n",
      "Epoch [265/1000] - Loss: 7.3191\n",
      "Epoch [266/1000] - Loss: 7.4802\n",
      "Epoch [267/1000] - Loss: 7.3098\n",
      "Epoch [268/1000] - Loss: 7.2546\n",
      "Epoch [269/1000] - Loss: 7.4390\n",
      "Epoch [270/1000] - Loss: 7.6827\n",
      "Epoch [271/1000] - Loss: 7.3576\n",
      "Epoch [272/1000] - Loss: 7.2294\n",
      "Epoch [273/1000] - Loss: 7.5084\n",
      "Epoch [274/1000] - Loss: 7.4033\n",
      "Epoch [275/1000] - Loss: 7.5292\n",
      "Epoch [276/1000] - Loss: 7.3689\n",
      "Epoch [277/1000] - Loss: 7.4851\n",
      "Epoch [278/1000] - Loss: 7.4679\n",
      "Epoch [279/1000] - Loss: 7.4792\n",
      "Epoch [280/1000] - Loss: 7.4261\n",
      "Epoch [281/1000] - Loss: 7.3525\n",
      "Epoch [282/1000] - Loss: 7.4656\n",
      "Epoch [283/1000] - Loss: 7.4498\n",
      "Epoch [284/1000] - Loss: 7.5038\n",
      "Epoch [285/1000] - Loss: 7.5987\n",
      "Epoch [286/1000] - Loss: 7.3254\n",
      "Epoch [287/1000] - Loss: 7.6143\n",
      "Epoch [288/1000] - Loss: 7.4385\n",
      "Epoch [289/1000] - Loss: 7.3643\n",
      "Epoch [290/1000] - Loss: 7.6123\n",
      "Epoch [291/1000] - Loss: 7.4862\n",
      "Epoch [292/1000] - Loss: 7.4026\n",
      "Epoch [293/1000] - Loss: 7.5979\n",
      "Epoch [294/1000] - Loss: 7.5284\n",
      "Epoch [295/1000] - Loss: 7.4717\n",
      "Epoch [296/1000] - Loss: 7.4246\n",
      "Epoch [297/1000] - Loss: 7.5960\n",
      "Epoch [298/1000] - Loss: 7.4729\n",
      "Epoch [299/1000] - Loss: 7.6965\n",
      "Epoch [300/1000] - Loss: 7.4348\n",
      "Epoch [301/1000] - Loss: 7.5122\n",
      "Epoch [302/1000] - Loss: 7.5479\n",
      "Epoch [303/1000] - Loss: 7.6376\n",
      "Epoch [304/1000] - Loss: 7.5227\n",
      "Epoch [305/1000] - Loss: 7.4128\n",
      "Epoch [306/1000] - Loss: 7.5986\n",
      "Epoch [307/1000] - Loss: 7.4757\n",
      "Epoch [308/1000] - Loss: 7.1477\n",
      "Epoch [309/1000] - Loss: 7.2994\n",
      "Epoch [310/1000] - Loss: 7.3077\n",
      "Epoch [311/1000] - Loss: 7.5386\n",
      "Epoch [312/1000] - Loss: 7.5981\n",
      "Epoch [313/1000] - Loss: 7.4001\n",
      "Epoch [314/1000] - Loss: 7.4761\n",
      "Epoch [315/1000] - Loss: 7.3399\n",
      "Epoch [316/1000] - Loss: 7.4474\n",
      "Epoch [317/1000] - Loss: 7.4638\n",
      "Epoch [318/1000] - Loss: 7.3144\n",
      "Epoch [319/1000] - Loss: 7.2670\n",
      "Epoch [320/1000] - Loss: 7.4781\n",
      "Epoch [321/1000] - Loss: 7.6286\n",
      "Epoch [322/1000] - Loss: 7.5915\n",
      "Epoch [323/1000] - Loss: 7.3286\n",
      "Epoch [324/1000] - Loss: 7.4708\n",
      "Epoch [325/1000] - Loss: 7.4243\n",
      "Epoch [326/1000] - Loss: 7.7983\n",
      "Epoch [327/1000] - Loss: 7.3700\n",
      "Epoch [328/1000] - Loss: 7.4576\n",
      "Epoch [329/1000] - Loss: 7.4629\n",
      "Epoch [330/1000] - Loss: 7.3756\n",
      "Epoch [331/1000] - Loss: 7.3965\n",
      "Epoch [332/1000] - Loss: 7.7142\n",
      "Epoch [333/1000] - Loss: 7.2726\n",
      "Epoch [334/1000] - Loss: 7.4146\n",
      "Epoch [335/1000] - Loss: 7.8436\n",
      "Epoch [336/1000] - Loss: 7.6311\n",
      "Epoch [337/1000] - Loss: 7.5027\n",
      "Epoch [338/1000] - Loss: 7.6548\n",
      "Epoch [339/1000] - Loss: 7.4909\n",
      "Epoch [340/1000] - Loss: 7.6779\n",
      "Epoch [341/1000] - Loss: 7.3987\n",
      "Epoch [342/1000] - Loss: 7.4295\n",
      "Epoch [343/1000] - Loss: 7.3999\n",
      "Epoch [344/1000] - Loss: 7.3677\n",
      "Epoch [345/1000] - Loss: 7.5417\n",
      "Epoch [346/1000] - Loss: 7.3992\n",
      "Epoch [347/1000] - Loss: 7.6605\n",
      "Epoch [348/1000] - Loss: 7.3966\n",
      "Epoch [349/1000] - Loss: 7.2269\n",
      "Epoch [350/1000] - Loss: 7.3015\n",
      "Epoch [351/1000] - Loss: 7.4459\n",
      "Epoch [352/1000] - Loss: 7.5006\n",
      "Epoch [353/1000] - Loss: 7.6018\n",
      "Epoch [354/1000] - Loss: 7.3062\n",
      "Epoch [355/1000] - Loss: 7.5962\n",
      "Epoch [356/1000] - Loss: 7.2393\n",
      "Epoch [357/1000] - Loss: 7.5779\n",
      "Epoch [358/1000] - Loss: 7.6433\n",
      "Epoch [359/1000] - Loss: 7.2668\n",
      "Epoch [360/1000] - Loss: 7.6000\n",
      "Epoch [361/1000] - Loss: 7.4547\n",
      "Epoch [362/1000] - Loss: 7.3724\n",
      "Epoch [363/1000] - Loss: 7.4145\n",
      "Epoch [364/1000] - Loss: 7.3576\n",
      "Epoch [365/1000] - Loss: 7.4481\n",
      "Epoch [366/1000] - Loss: 7.4099\n",
      "Epoch [367/1000] - Loss: 7.3645\n",
      "Epoch [368/1000] - Loss: 7.4612\n",
      "Epoch [369/1000] - Loss: 7.3570\n",
      "Epoch [370/1000] - Loss: 7.4824\n",
      "Epoch [371/1000] - Loss: 7.5445\n",
      "Epoch [372/1000] - Loss: 7.3952\n",
      "Epoch [373/1000] - Loss: 7.3363\n",
      "Epoch [374/1000] - Loss: 7.4971\n",
      "Epoch [375/1000] - Loss: 7.3130\n",
      "Epoch [376/1000] - Loss: 7.3329\n",
      "Epoch [377/1000] - Loss: 7.1361\n",
      "Epoch [378/1000] - Loss: 7.4261\n",
      "Epoch [379/1000] - Loss: 7.7258\n",
      "Epoch [380/1000] - Loss: 7.2922\n",
      "Epoch [381/1000] - Loss: 7.4046\n",
      "Epoch [382/1000] - Loss: 7.5189\n",
      "Epoch [383/1000] - Loss: 7.6403\n",
      "Epoch [384/1000] - Loss: 7.5244\n",
      "Epoch [385/1000] - Loss: 7.4114\n",
      "Epoch [386/1000] - Loss: 7.5288\n",
      "Epoch [387/1000] - Loss: 7.5479\n",
      "Epoch [388/1000] - Loss: 7.4727\n",
      "Epoch [389/1000] - Loss: 7.5623\n",
      "Epoch [390/1000] - Loss: 7.4436\n",
      "Epoch [391/1000] - Loss: 7.3171\n",
      "Epoch [392/1000] - Loss: 7.5350\n",
      "Epoch [393/1000] - Loss: 7.4553\n",
      "Epoch [394/1000] - Loss: 7.0222\n",
      "Epoch [395/1000] - Loss: 7.4697\n",
      "Epoch [396/1000] - Loss: 7.6248\n",
      "Epoch [397/1000] - Loss: 7.5070\n",
      "Epoch [398/1000] - Loss: 7.6144\n",
      "Epoch [399/1000] - Loss: 7.5067\n",
      "Epoch [400/1000] - Loss: 7.6501\n",
      "Epoch [401/1000] - Loss: 7.5211\n",
      "Epoch [402/1000] - Loss: 7.3522\n",
      "Epoch [403/1000] - Loss: 7.4745\n",
      "Epoch [404/1000] - Loss: 7.4169\n",
      "Epoch [405/1000] - Loss: 7.4776\n",
      "Epoch [406/1000] - Loss: 7.3115\n",
      "Epoch [407/1000] - Loss: 7.1964\n",
      "Epoch [408/1000] - Loss: 7.4051\n",
      "Epoch [409/1000] - Loss: 7.3857\n",
      "Epoch [410/1000] - Loss: 7.4989\n",
      "Epoch [411/1000] - Loss: 7.3688\n",
      "Epoch [412/1000] - Loss: 7.1978\n",
      "Epoch [413/1000] - Loss: 7.2326\n",
      "Epoch [414/1000] - Loss: 7.7086\n",
      "Epoch [415/1000] - Loss: 7.5476\n",
      "Epoch [416/1000] - Loss: 7.4893\n",
      "Epoch [417/1000] - Loss: 7.5358\n",
      "Epoch [418/1000] - Loss: 7.4650\n",
      "Epoch [419/1000] - Loss: 7.5385\n",
      "Epoch [420/1000] - Loss: 7.4284\n",
      "Epoch [421/1000] - Loss: 7.6960\n",
      "Epoch [422/1000] - Loss: 7.5741\n",
      "Epoch [423/1000] - Loss: 7.3849\n",
      "Epoch [424/1000] - Loss: 7.2116\n",
      "Epoch [425/1000] - Loss: 7.2181\n",
      "Epoch [426/1000] - Loss: 7.4293\n",
      "Epoch [427/1000] - Loss: 7.4426\n",
      "Epoch [428/1000] - Loss: 7.2776\n",
      "Epoch [429/1000] - Loss: 7.4068\n",
      "Epoch [430/1000] - Loss: 7.5054\n",
      "Epoch [431/1000] - Loss: 7.3179\n",
      "Epoch [432/1000] - Loss: 7.5185\n",
      "Epoch [433/1000] - Loss: 7.1954\n",
      "Epoch [434/1000] - Loss: 7.2382\n",
      "Epoch [435/1000] - Loss: 7.4188\n",
      "Epoch [436/1000] - Loss: 7.4374\n",
      "Epoch [437/1000] - Loss: 7.5371\n",
      "Epoch [438/1000] - Loss: 7.6340\n",
      "Epoch [439/1000] - Loss: 7.4191\n",
      "Epoch [440/1000] - Loss: 7.1201\n",
      "Epoch [441/1000] - Loss: 7.2462\n",
      "Epoch [442/1000] - Loss: 7.2475\n",
      "Epoch [443/1000] - Loss: 7.3284\n",
      "Epoch [444/1000] - Loss: 7.4389\n",
      "Epoch [445/1000] - Loss: 7.5200\n",
      "Epoch [446/1000] - Loss: 7.1750\n",
      "Epoch [447/1000] - Loss: 7.1776\n",
      "Epoch [448/1000] - Loss: 7.3920\n",
      "Epoch [449/1000] - Loss: 7.5309\n",
      "Epoch [450/1000] - Loss: 7.5377\n",
      "Epoch [451/1000] - Loss: 7.5760\n",
      "Epoch [452/1000] - Loss: 7.5846\n",
      "Epoch [453/1000] - Loss: 7.4561\n",
      "Epoch [454/1000] - Loss: 7.1696\n",
      "Epoch [455/1000] - Loss: 7.2944\n",
      "Epoch [456/1000] - Loss: 7.3660\n",
      "Epoch [457/1000] - Loss: 7.6694\n",
      "Epoch [458/1000] - Loss: 7.6455\n",
      "Epoch [459/1000] - Loss: 7.4701\n",
      "Epoch [460/1000] - Loss: 7.4017\n",
      "Epoch [461/1000] - Loss: 7.3998\n",
      "Epoch [462/1000] - Loss: 7.5611\n",
      "Epoch [463/1000] - Loss: 7.4257\n",
      "Epoch [464/1000] - Loss: 7.5388\n",
      "Epoch [465/1000] - Loss: 7.6366\n",
      "Epoch [466/1000] - Loss: 7.4571\n",
      "Epoch [467/1000] - Loss: 7.5613\n",
      "Epoch [468/1000] - Loss: 7.6341\n",
      "Epoch [469/1000] - Loss: 7.3448\n",
      "Epoch [470/1000] - Loss: 7.7444\n",
      "Epoch [471/1000] - Loss: 7.4032\n",
      "Epoch [472/1000] - Loss: 7.3219\n",
      "Epoch [473/1000] - Loss: 7.5348\n",
      "Epoch [474/1000] - Loss: 7.4053\n",
      "Epoch [475/1000] - Loss: 7.2537\n",
      "Epoch [476/1000] - Loss: 7.4284\n",
      "Epoch [477/1000] - Loss: 7.4915\n",
      "Epoch [478/1000] - Loss: 7.3400\n",
      "Epoch [479/1000] - Loss: 7.5251\n",
      "Epoch [480/1000] - Loss: 7.4504\n",
      "Epoch [481/1000] - Loss: 7.4692\n",
      "Epoch [482/1000] - Loss: 7.5874\n",
      "Epoch [483/1000] - Loss: 7.5804\n",
      "Epoch [484/1000] - Loss: 7.4360\n",
      "Epoch [485/1000] - Loss: 7.4372\n",
      "Epoch [486/1000] - Loss: 7.2265\n",
      "Epoch [487/1000] - Loss: 7.4592\n",
      "Epoch [488/1000] - Loss: 7.5674\n",
      "Epoch [489/1000] - Loss: 7.3765\n",
      "Epoch [490/1000] - Loss: 7.4429\n",
      "Epoch [491/1000] - Loss: 7.5558\n",
      "Epoch [492/1000] - Loss: 7.2686\n",
      "Epoch [493/1000] - Loss: 7.5807\n",
      "Epoch [494/1000] - Loss: 7.2816\n",
      "Epoch [495/1000] - Loss: 7.2380\n",
      "Epoch [496/1000] - Loss: 7.2552\n",
      "Epoch [497/1000] - Loss: 7.3792\n",
      "Epoch [498/1000] - Loss: 7.5460\n",
      "Epoch [499/1000] - Loss: 7.4133\n",
      "Epoch [500/1000] - Loss: 7.4707\n",
      "Epoch [501/1000] - Loss: 7.3965\n",
      "Epoch [502/1000] - Loss: 7.4981\n",
      "Epoch [503/1000] - Loss: 7.4077\n",
      "Epoch [504/1000] - Loss: 7.5346\n",
      "Epoch [505/1000] - Loss: 7.5373\n",
      "Epoch [506/1000] - Loss: 7.3716\n",
      "Epoch [507/1000] - Loss: 7.5960\n",
      "Epoch [508/1000] - Loss: 7.5868\n",
      "Epoch [509/1000] - Loss: 7.4153\n",
      "Epoch [510/1000] - Loss: 7.4594\n",
      "Epoch [511/1000] - Loss: 7.3497\n",
      "Epoch [512/1000] - Loss: 7.3899\n",
      "Epoch [513/1000] - Loss: 7.6482\n",
      "Epoch [514/1000] - Loss: 7.3532\n",
      "Epoch [515/1000] - Loss: 7.3188\n",
      "Epoch [516/1000] - Loss: 7.3389\n",
      "Epoch [517/1000] - Loss: 7.4313\n",
      "Epoch [518/1000] - Loss: 7.1165\n",
      "Epoch [519/1000] - Loss: 7.3326\n",
      "Epoch [520/1000] - Loss: 7.4150\n",
      "Epoch [521/1000] - Loss: 7.4126\n",
      "Epoch [522/1000] - Loss: 7.5558\n",
      "Epoch [523/1000] - Loss: 7.3341\n",
      "Epoch [524/1000] - Loss: 7.5567\n",
      "Epoch [525/1000] - Loss: 7.3392\n",
      "Epoch [526/1000] - Loss: 7.6242\n",
      "Epoch [527/1000] - Loss: 7.4396\n",
      "Epoch [528/1000] - Loss: 7.5145\n",
      "Epoch [529/1000] - Loss: 7.4693\n",
      "Epoch [530/1000] - Loss: 7.5086\n",
      "Epoch [531/1000] - Loss: 7.6213\n",
      "Epoch [532/1000] - Loss: 7.5286\n",
      "Epoch [533/1000] - Loss: 7.5403\n",
      "Epoch [534/1000] - Loss: 7.5591\n",
      "Epoch [535/1000] - Loss: 7.2988\n",
      "Epoch [536/1000] - Loss: 7.4707\n",
      "Epoch [537/1000] - Loss: 7.5338\n",
      "Epoch [538/1000] - Loss: 7.3317\n",
      "Epoch [539/1000] - Loss: 7.6460\n",
      "Epoch [540/1000] - Loss: 7.5365\n",
      "Epoch [541/1000] - Loss: 7.1910\n",
      "Epoch [542/1000] - Loss: 7.2166\n",
      "Epoch [543/1000] - Loss: 7.5723\n",
      "Epoch [544/1000] - Loss: 7.6252\n",
      "Epoch [545/1000] - Loss: 7.4593\n",
      "Epoch [546/1000] - Loss: 7.2686\n",
      "Epoch [547/1000] - Loss: 7.5306\n",
      "Epoch [548/1000] - Loss: 7.5104\n",
      "Epoch [549/1000] - Loss: 7.4578\n",
      "Epoch [550/1000] - Loss: 7.7367\n",
      "Epoch [551/1000] - Loss: 7.4102\n",
      "Epoch [552/1000] - Loss: 7.3004\n",
      "Epoch [553/1000] - Loss: 7.3289\n",
      "Epoch [554/1000] - Loss: 7.4387\n",
      "Epoch [555/1000] - Loss: 7.3110\n",
      "Epoch [556/1000] - Loss: 7.4617\n",
      "Epoch [557/1000] - Loss: 7.6328\n",
      "Epoch [558/1000] - Loss: 7.6017\n",
      "Epoch [559/1000] - Loss: 7.4452\n",
      "Epoch [560/1000] - Loss: 7.2842\n",
      "Epoch [561/1000] - Loss: 7.3777\n",
      "Epoch [562/1000] - Loss: 7.4236\n",
      "Epoch [563/1000] - Loss: 7.5540\n",
      "Epoch [564/1000] - Loss: 7.4453\n",
      "Epoch [565/1000] - Loss: 7.4220\n",
      "Epoch [566/1000] - Loss: 7.6209\n",
      "Epoch [567/1000] - Loss: 7.5602\n",
      "Epoch [568/1000] - Loss: 7.1224\n",
      "Epoch [569/1000] - Loss: 7.3367\n",
      "Epoch [570/1000] - Loss: 7.4945\n",
      "Epoch [571/1000] - Loss: 7.5172\n",
      "Epoch [572/1000] - Loss: 7.3571\n",
      "Epoch [573/1000] - Loss: 7.1836\n",
      "Epoch [574/1000] - Loss: 7.5682\n",
      "Epoch [575/1000] - Loss: 7.2571\n",
      "Epoch [576/1000] - Loss: 7.4744\n",
      "Epoch [577/1000] - Loss: 7.5143\n",
      "Epoch [578/1000] - Loss: 7.3574\n",
      "Epoch [579/1000] - Loss: 7.4276\n",
      "Epoch [580/1000] - Loss: 7.3239\n",
      "Epoch [581/1000] - Loss: 7.6457\n",
      "Epoch [582/1000] - Loss: 7.6016\n",
      "Epoch [583/1000] - Loss: 7.4269\n",
      "Epoch [584/1000] - Loss: 7.6625\n",
      "Epoch [585/1000] - Loss: 7.5420\n",
      "Epoch [586/1000] - Loss: 7.5418\n",
      "Epoch [587/1000] - Loss: 7.4879\n",
      "Epoch [588/1000] - Loss: 7.5709\n",
      "Epoch [589/1000] - Loss: 7.2905\n",
      "Epoch [590/1000] - Loss: 7.4847\n",
      "Epoch [591/1000] - Loss: 7.4427\n",
      "Epoch [592/1000] - Loss: 7.4508\n",
      "Epoch [593/1000] - Loss: 7.4322\n",
      "Epoch [594/1000] - Loss: 7.5131\n",
      "Epoch [595/1000] - Loss: 7.3666\n",
      "Epoch [596/1000] - Loss: 7.5383\n",
      "Epoch [597/1000] - Loss: 7.2570\n",
      "Epoch [598/1000] - Loss: 7.3931\n",
      "Epoch [599/1000] - Loss: 7.5424\n",
      "Epoch [600/1000] - Loss: 7.4421\n",
      "Epoch [601/1000] - Loss: 7.2357\n",
      "Epoch [602/1000] - Loss: 7.3592\n",
      "Epoch [603/1000] - Loss: 7.2366\n",
      "Epoch [604/1000] - Loss: 7.4775\n",
      "Epoch [605/1000] - Loss: 7.6250\n",
      "Epoch [606/1000] - Loss: 7.5664\n",
      "Epoch [607/1000] - Loss: 7.3457\n",
      "Epoch [608/1000] - Loss: 7.1515\n",
      "Epoch [609/1000] - Loss: 7.4919\n",
      "Epoch [610/1000] - Loss: 7.3948\n",
      "Epoch [611/1000] - Loss: 7.2696\n",
      "Epoch [612/1000] - Loss: 7.2456\n",
      "Epoch [613/1000] - Loss: 7.4470\n",
      "Epoch [614/1000] - Loss: 7.6167\n",
      "Epoch [615/1000] - Loss: 7.0847\n",
      "Epoch [616/1000] - Loss: 7.3492\n",
      "Epoch [617/1000] - Loss: 7.5243\n",
      "Epoch [618/1000] - Loss: 7.2931\n",
      "Epoch [619/1000] - Loss: 7.5875\n",
      "Epoch [620/1000] - Loss: 7.2825\n",
      "Epoch [621/1000] - Loss: 7.5372\n",
      "Epoch [622/1000] - Loss: 7.3572\n",
      "Epoch [623/1000] - Loss: 7.6090\n",
      "Epoch [624/1000] - Loss: 7.5123\n",
      "Epoch [625/1000] - Loss: 7.6756\n",
      "Epoch [626/1000] - Loss: 7.3810\n",
      "Epoch [627/1000] - Loss: 7.4350\n",
      "Epoch [628/1000] - Loss: 7.2688\n",
      "Epoch [629/1000] - Loss: 7.4797\n",
      "Epoch [630/1000] - Loss: 7.4170\n",
      "Epoch [631/1000] - Loss: 7.3682\n",
      "Epoch [632/1000] - Loss: 7.3980\n",
      "Epoch [633/1000] - Loss: 7.5978\n",
      "Epoch [634/1000] - Loss: 7.1988\n",
      "Epoch [635/1000] - Loss: 7.5044\n",
      "Epoch [636/1000] - Loss: 7.3988\n",
      "Epoch [637/1000] - Loss: 7.1748\n",
      "Epoch [638/1000] - Loss: 7.3781\n",
      "Epoch [639/1000] - Loss: 7.1247\n",
      "Epoch [640/1000] - Loss: 7.5843\n",
      "Epoch [641/1000] - Loss: 7.2368\n",
      "Epoch [642/1000] - Loss: 7.6166\n",
      "Epoch [643/1000] - Loss: 7.4711\n",
      "Epoch [644/1000] - Loss: 7.4977\n",
      "Epoch [645/1000] - Loss: 7.2522\n",
      "Epoch [646/1000] - Loss: 7.5308\n",
      "Epoch [647/1000] - Loss: 7.1451\n",
      "Epoch [648/1000] - Loss: 7.3685\n",
      "Epoch [649/1000] - Loss: 7.4611\n",
      "Epoch [650/1000] - Loss: 7.5831\n",
      "Epoch [651/1000] - Loss: 7.3820\n",
      "Epoch [652/1000] - Loss: 7.4143\n",
      "Epoch [653/1000] - Loss: 7.3743\n",
      "Epoch [654/1000] - Loss: 7.4008\n",
      "Epoch [655/1000] - Loss: 7.4362\n",
      "Epoch [656/1000] - Loss: 7.6130\n",
      "Epoch [657/1000] - Loss: 7.2943\n",
      "Epoch [658/1000] - Loss: 7.3691\n",
      "Epoch [659/1000] - Loss: 7.3154\n",
      "Epoch [660/1000] - Loss: 7.2641\n",
      "Epoch [661/1000] - Loss: 7.5354\n",
      "Epoch [662/1000] - Loss: 7.2727\n",
      "Epoch [663/1000] - Loss: 7.3992\n",
      "Epoch [664/1000] - Loss: 7.7061\n",
      "Epoch [665/1000] - Loss: 7.3128\n",
      "Epoch [666/1000] - Loss: 7.3904\n",
      "Epoch [667/1000] - Loss: 7.3703\n",
      "Epoch [668/1000] - Loss: 7.4080\n",
      "Epoch [669/1000] - Loss: 7.5729\n",
      "Epoch [670/1000] - Loss: 7.4603\n",
      "Epoch [671/1000] - Loss: 7.5031\n",
      "Epoch [672/1000] - Loss: 7.2435\n",
      "Epoch [673/1000] - Loss: 7.2767\n",
      "Epoch [674/1000] - Loss: 7.5594\n",
      "Epoch [675/1000] - Loss: 7.3619\n",
      "Epoch [676/1000] - Loss: 7.2212\n",
      "Epoch [677/1000] - Loss: 7.3303\n",
      "Epoch [678/1000] - Loss: 7.3727\n",
      "Epoch [679/1000] - Loss: 7.6196\n",
      "Epoch [680/1000] - Loss: 7.4271\n",
      "Epoch [681/1000] - Loss: 7.4901\n",
      "Epoch [682/1000] - Loss: 7.5636\n",
      "Epoch [683/1000] - Loss: 7.3975\n",
      "Epoch [684/1000] - Loss: 7.3700\n",
      "Epoch [685/1000] - Loss: 7.4235\n",
      "Epoch [686/1000] - Loss: 7.4769\n",
      "Epoch [687/1000] - Loss: 7.3485\n",
      "Epoch [688/1000] - Loss: 7.3515\n",
      "Epoch [689/1000] - Loss: 7.2039\n",
      "Epoch [690/1000] - Loss: 7.3969\n",
      "Epoch [691/1000] - Loss: 7.3390\n",
      "Epoch [692/1000] - Loss: 7.4020\n",
      "Epoch [693/1000] - Loss: 7.5699\n",
      "Epoch [694/1000] - Loss: 7.7597\n",
      "Epoch [695/1000] - Loss: 7.5118\n",
      "Epoch [696/1000] - Loss: 7.4518\n",
      "Epoch [697/1000] - Loss: 7.1877\n",
      "Epoch [698/1000] - Loss: 7.3623\n",
      "Epoch [699/1000] - Loss: 7.5800\n",
      "Epoch [700/1000] - Loss: 7.3355\n",
      "Epoch [701/1000] - Loss: 7.5596\n",
      "Epoch [702/1000] - Loss: 7.3690\n",
      "Epoch [703/1000] - Loss: 7.5338\n",
      "Epoch [704/1000] - Loss: 7.5000\n",
      "Epoch [705/1000] - Loss: 7.1927\n",
      "Epoch [706/1000] - Loss: 7.5495\n",
      "Epoch [707/1000] - Loss: 7.5229\n",
      "Epoch [708/1000] - Loss: 7.2386\n",
      "Epoch [709/1000] - Loss: 7.6808\n",
      "Epoch [710/1000] - Loss: 7.1067\n",
      "Epoch [711/1000] - Loss: 7.3111\n",
      "Epoch [712/1000] - Loss: 7.5017\n",
      "Epoch [713/1000] - Loss: 7.3064\n",
      "Epoch [714/1000] - Loss: 7.2937\n",
      "Epoch [715/1000] - Loss: 7.3625\n",
      "Epoch [716/1000] - Loss: 7.4811\n",
      "Epoch [717/1000] - Loss: 7.3955\n",
      "Epoch [718/1000] - Loss: 7.3114\n",
      "Epoch [719/1000] - Loss: 7.4332\n",
      "Epoch [720/1000] - Loss: 7.4307\n",
      "Epoch [721/1000] - Loss: 7.2843\n",
      "Epoch [722/1000] - Loss: 7.5892\n",
      "Epoch [723/1000] - Loss: 7.3263\n",
      "Epoch [724/1000] - Loss: 7.1975\n",
      "Epoch [725/1000] - Loss: 7.3873\n",
      "Epoch [726/1000] - Loss: 7.2612\n",
      "Epoch [727/1000] - Loss: 7.4050\n",
      "Epoch [728/1000] - Loss: 7.5233\n",
      "Epoch [729/1000] - Loss: 7.2750\n",
      "Epoch [730/1000] - Loss: 7.3478\n",
      "Epoch [731/1000] - Loss: 7.3219\n",
      "Epoch [732/1000] - Loss: 7.6083\n",
      "Epoch [733/1000] - Loss: 7.2979\n",
      "Epoch [734/1000] - Loss: 7.5276\n",
      "Epoch [735/1000] - Loss: 7.6515\n",
      "Epoch [736/1000] - Loss: 7.5704\n",
      "Epoch [737/1000] - Loss: 7.3510\n",
      "Epoch [738/1000] - Loss: 7.2407\n",
      "Epoch [739/1000] - Loss: 7.3306\n",
      "Epoch [740/1000] - Loss: 7.6137\n",
      "Epoch [741/1000] - Loss: 7.4839\n",
      "Epoch [742/1000] - Loss: 7.9867\n",
      "Epoch [743/1000] - Loss: 7.3128\n",
      "Epoch [744/1000] - Loss: 7.3347\n",
      "Epoch [745/1000] - Loss: 7.4963\n",
      "Epoch [746/1000] - Loss: 7.5582\n",
      "Epoch [747/1000] - Loss: 7.3856\n",
      "Epoch [748/1000] - Loss: 7.3630\n",
      "Epoch [749/1000] - Loss: 7.4172\n",
      "Epoch [750/1000] - Loss: 7.2818\n",
      "Epoch [751/1000] - Loss: 7.4791\n",
      "Epoch [752/1000] - Loss: 7.3570\n",
      "Epoch [753/1000] - Loss: 7.3623\n",
      "Epoch [754/1000] - Loss: 7.5581\n",
      "Epoch [755/1000] - Loss: 7.6549\n",
      "Epoch [756/1000] - Loss: 7.3749\n",
      "Epoch [757/1000] - Loss: 7.4927\n",
      "Epoch [758/1000] - Loss: 7.2304\n",
      "Epoch [759/1000] - Loss: 7.2822\n",
      "Epoch [760/1000] - Loss: 7.3681\n",
      "Epoch [761/1000] - Loss: 7.3839\n",
      "Epoch [762/1000] - Loss: 7.6259\n",
      "Epoch [763/1000] - Loss: 7.4246\n",
      "Epoch [764/1000] - Loss: 7.5705\n",
      "Epoch [765/1000] - Loss: 7.5652\n",
      "Epoch [766/1000] - Loss: 7.3239\n",
      "Epoch [767/1000] - Loss: 7.2959\n",
      "Epoch [768/1000] - Loss: 7.4720\n",
      "Epoch [769/1000] - Loss: 7.3468\n",
      "Epoch [770/1000] - Loss: 7.2861\n",
      "Epoch [771/1000] - Loss: 7.1693\n",
      "Epoch [772/1000] - Loss: 7.3917\n",
      "Epoch [773/1000] - Loss: 7.2634\n",
      "Epoch [774/1000] - Loss: 7.6353\n",
      "Epoch [775/1000] - Loss: 7.3302\n",
      "Epoch [776/1000] - Loss: 7.6389\n",
      "Epoch [777/1000] - Loss: 7.4309\n",
      "Epoch [778/1000] - Loss: 7.5361\n",
      "Epoch [779/1000] - Loss: 7.3116\n",
      "Epoch [780/1000] - Loss: 7.7090\n",
      "Epoch [781/1000] - Loss: 7.2629\n",
      "Epoch [782/1000] - Loss: 7.3110\n",
      "Epoch [783/1000] - Loss: 7.3452\n",
      "Epoch [784/1000] - Loss: 7.4077\n",
      "Epoch [785/1000] - Loss: 7.4494\n",
      "Epoch [786/1000] - Loss: 7.7454\n",
      "Epoch [787/1000] - Loss: 7.2280\n",
      "Epoch [788/1000] - Loss: 7.3388\n",
      "Epoch [789/1000] - Loss: 7.4110\n",
      "Epoch [790/1000] - Loss: 7.4135\n",
      "Epoch [791/1000] - Loss: 7.4398\n",
      "Epoch [792/1000] - Loss: 7.6813\n",
      "Epoch [793/1000] - Loss: 7.3591\n",
      "Epoch [794/1000] - Loss: 7.2886\n",
      "Epoch [795/1000] - Loss: 7.3568\n",
      "Epoch [796/1000] - Loss: 7.5447\n",
      "Epoch [797/1000] - Loss: 7.4972\n",
      "Epoch [798/1000] - Loss: 7.3576\n",
      "Epoch [799/1000] - Loss: 7.8047\n",
      "Epoch [800/1000] - Loss: 7.3884\n",
      "Epoch [801/1000] - Loss: 7.5499\n",
      "Epoch [802/1000] - Loss: 7.3026\n",
      "Epoch [803/1000] - Loss: 7.3191\n",
      "Epoch [804/1000] - Loss: 7.1698\n",
      "Epoch [805/1000] - Loss: 7.3640\n",
      "Epoch [806/1000] - Loss: 7.3691\n",
      "Epoch [807/1000] - Loss: 7.3697\n",
      "Epoch [808/1000] - Loss: 7.3171\n",
      "Epoch [809/1000] - Loss: 7.3965\n",
      "Epoch [810/1000] - Loss: 7.7419\n",
      "Epoch [811/1000] - Loss: 7.4226\n",
      "Epoch [812/1000] - Loss: 7.5329\n",
      "Epoch [813/1000] - Loss: 7.5242\n",
      "Epoch [814/1000] - Loss: 7.1773\n",
      "Epoch [815/1000] - Loss: 7.1072\n",
      "Epoch [816/1000] - Loss: 7.5352\n",
      "Epoch [817/1000] - Loss: 7.1730\n",
      "Epoch [818/1000] - Loss: 7.3546\n",
      "Epoch [819/1000] - Loss: 7.1810\n",
      "Epoch [820/1000] - Loss: 7.2943\n",
      "Epoch [821/1000] - Loss: 7.2743\n",
      "Epoch [822/1000] - Loss: 7.3387\n",
      "Epoch [823/1000] - Loss: 7.6005\n",
      "Epoch [824/1000] - Loss: 7.4445\n",
      "Epoch [825/1000] - Loss: 7.1655\n",
      "Epoch [826/1000] - Loss: 7.5027\n",
      "Epoch [827/1000] - Loss: 7.3437\n",
      "Epoch [828/1000] - Loss: 7.4232\n",
      "Epoch [829/1000] - Loss: 7.4431\n",
      "Epoch [830/1000] - Loss: 7.2352\n",
      "Epoch [831/1000] - Loss: 7.3678\n",
      "Epoch [832/1000] - Loss: 7.3062\n",
      "Epoch [833/1000] - Loss: 7.5326\n",
      "Epoch [834/1000] - Loss: 7.3768\n",
      "Epoch [835/1000] - Loss: 7.1897\n",
      "Epoch [836/1000] - Loss: 7.2784\n",
      "Epoch [837/1000] - Loss: 7.5548\n",
      "Epoch [838/1000] - Loss: 7.4750\n",
      "Epoch [839/1000] - Loss: 7.2792\n",
      "Epoch [840/1000] - Loss: 7.5383\n",
      "Epoch [841/1000] - Loss: 7.4754\n",
      "Epoch [842/1000] - Loss: 7.5884\n",
      "Epoch [843/1000] - Loss: 7.2893\n",
      "Epoch [844/1000] - Loss: 7.3964\n",
      "Epoch [845/1000] - Loss: 7.5172\n",
      "Epoch [846/1000] - Loss: 7.1990\n",
      "Epoch [847/1000] - Loss: 7.4582\n",
      "Epoch [848/1000] - Loss: 7.2282\n",
      "Epoch [849/1000] - Loss: 7.3549\n",
      "Epoch [850/1000] - Loss: 7.1990\n",
      "Epoch [851/1000] - Loss: 7.2313\n",
      "Epoch [852/1000] - Loss: 7.4113\n",
      "Epoch [853/1000] - Loss: 7.2406\n",
      "Epoch [854/1000] - Loss: 7.5084\n",
      "Epoch [855/1000] - Loss: 7.4980\n",
      "Epoch [856/1000] - Loss: 7.4153\n",
      "Epoch [857/1000] - Loss: 7.3958\n",
      "Epoch [858/1000] - Loss: 7.3053\n",
      "Epoch [859/1000] - Loss: 7.4150\n",
      "Epoch [860/1000] - Loss: 7.4424\n",
      "Epoch [861/1000] - Loss: 7.5120\n",
      "Epoch [862/1000] - Loss: 7.4914\n",
      "Epoch [863/1000] - Loss: 7.6120\n",
      "Epoch [864/1000] - Loss: 7.3132\n",
      "Epoch [865/1000] - Loss: 7.3828\n",
      "Epoch [866/1000] - Loss: 7.5390\n",
      "Epoch [867/1000] - Loss: 7.4405\n",
      "Epoch [868/1000] - Loss: 7.3040\n",
      "Epoch [869/1000] - Loss: 7.2362\n",
      "Epoch [870/1000] - Loss: 7.2700\n",
      "Epoch [871/1000] - Loss: 7.3584\n",
      "Epoch [872/1000] - Loss: 7.3027\n",
      "Epoch [873/1000] - Loss: 7.3820\n",
      "Epoch [874/1000] - Loss: 7.3626\n",
      "Epoch [875/1000] - Loss: 7.3159\n",
      "Epoch [876/1000] - Loss: 7.3593\n",
      "Epoch [877/1000] - Loss: 7.5478\n",
      "Epoch [878/1000] - Loss: 7.3752\n",
      "Epoch [879/1000] - Loss: 7.3588\n",
      "Epoch [880/1000] - Loss: 7.1759\n",
      "Epoch [881/1000] - Loss: 7.1351\n",
      "Epoch [882/1000] - Loss: 7.1860\n",
      "Epoch [883/1000] - Loss: 7.1513\n",
      "Epoch [884/1000] - Loss: 7.1688\n",
      "Epoch [885/1000] - Loss: 7.1837\n",
      "Epoch [886/1000] - Loss: 7.4016\n",
      "Epoch [887/1000] - Loss: 7.4444\n",
      "Epoch [888/1000] - Loss: 7.6045\n",
      "Epoch [889/1000] - Loss: 7.3319\n",
      "Epoch [890/1000] - Loss: 7.3759\n",
      "Epoch [891/1000] - Loss: 7.2590\n",
      "Epoch [892/1000] - Loss: 7.6097\n",
      "Epoch [893/1000] - Loss: 7.4639\n",
      "Epoch [894/1000] - Loss: 7.5157\n",
      "Epoch [895/1000] - Loss: 7.5867\n",
      "Epoch [896/1000] - Loss: 7.2929\n",
      "Epoch [897/1000] - Loss: 7.6510\n",
      "Epoch [898/1000] - Loss: 7.5430\n",
      "Epoch [899/1000] - Loss: 7.5270\n",
      "Epoch [900/1000] - Loss: 7.5896\n",
      "Epoch [901/1000] - Loss: 7.5613\n",
      "Epoch [902/1000] - Loss: 7.4943\n",
      "Epoch [903/1000] - Loss: 7.5546\n",
      "Epoch [904/1000] - Loss: 7.3445\n",
      "Epoch [905/1000] - Loss: 7.3816\n",
      "Epoch [906/1000] - Loss: 7.4346\n",
      "Epoch [907/1000] - Loss: 7.5703\n",
      "Epoch [908/1000] - Loss: 7.2072\n",
      "Epoch [909/1000] - Loss: 7.4083\n",
      "Epoch [910/1000] - Loss: 7.3256\n",
      "Epoch [911/1000] - Loss: 7.5064\n",
      "Epoch [912/1000] - Loss: 7.3979\n",
      "Epoch [913/1000] - Loss: 7.4917\n",
      "Epoch [914/1000] - Loss: 7.3375\n",
      "Epoch [915/1000] - Loss: 7.5148\n",
      "Epoch [916/1000] - Loss: 7.3780\n",
      "Epoch [917/1000] - Loss: 7.2156\n",
      "Epoch [918/1000] - Loss: 7.4665\n",
      "Epoch [919/1000] - Loss: 7.4102\n",
      "Epoch [920/1000] - Loss: 7.4637\n",
      "Epoch [921/1000] - Loss: 7.2403\n",
      "Epoch [922/1000] - Loss: 7.6648\n",
      "Epoch [923/1000] - Loss: 7.3416\n",
      "Epoch [924/1000] - Loss: 7.5417\n",
      "Epoch [925/1000] - Loss: 7.5141\n",
      "Epoch [926/1000] - Loss: 7.0550\n",
      "Epoch [927/1000] - Loss: 7.4436\n",
      "Epoch [928/1000] - Loss: 7.3808\n",
      "Epoch [929/1000] - Loss: 7.1929\n",
      "Epoch [930/1000] - Loss: 7.2843\n",
      "Epoch [931/1000] - Loss: 7.3145\n",
      "Epoch [932/1000] - Loss: 7.2927\n",
      "Epoch [933/1000] - Loss: 7.4991\n",
      "Epoch [934/1000] - Loss: 7.4684\n",
      "Epoch [935/1000] - Loss: 7.6667\n",
      "Epoch [936/1000] - Loss: 7.3807\n",
      "Epoch [937/1000] - Loss: 7.6010\n",
      "Epoch [938/1000] - Loss: 7.1451\n",
      "Epoch [939/1000] - Loss: 7.4947\n",
      "Epoch [940/1000] - Loss: 7.2912\n",
      "Epoch [941/1000] - Loss: 7.4388\n",
      "Epoch [942/1000] - Loss: 7.6737\n",
      "Epoch [943/1000] - Loss: 7.4642\n",
      "Epoch [944/1000] - Loss: 7.4204\n",
      "Epoch [945/1000] - Loss: 7.3377\n",
      "Epoch [946/1000] - Loss: 7.3352\n",
      "Epoch [947/1000] - Loss: 7.5083\n",
      "Epoch [948/1000] - Loss: 7.6384\n",
      "Epoch [949/1000] - Loss: 7.4004\n",
      "Epoch [950/1000] - Loss: 7.2426\n",
      "Epoch [951/1000] - Loss: 7.5849\n",
      "Epoch [952/1000] - Loss: 7.4957\n",
      "Epoch [953/1000] - Loss: 7.1096\n",
      "Epoch [954/1000] - Loss: 7.2969\n",
      "Epoch [955/1000] - Loss: 7.4141\n",
      "Epoch [956/1000] - Loss: 7.4960\n",
      "Epoch [957/1000] - Loss: 7.3756\n",
      "Epoch [958/1000] - Loss: 7.3454\n",
      "Epoch [959/1000] - Loss: 7.5312\n",
      "Epoch [960/1000] - Loss: 7.5066\n",
      "Epoch [961/1000] - Loss: 7.3412\n",
      "Epoch [962/1000] - Loss: 7.5875\n",
      "Epoch [963/1000] - Loss: 7.1539\n",
      "Epoch [964/1000] - Loss: 7.3469\n",
      "Epoch [965/1000] - Loss: 7.2274\n",
      "Epoch [966/1000] - Loss: 7.3840\n",
      "Epoch [967/1000] - Loss: 7.4142\n",
      "Epoch [968/1000] - Loss: 7.7974\n",
      "Epoch [969/1000] - Loss: 7.1936\n",
      "Epoch [970/1000] - Loss: 7.3692\n",
      "Epoch [971/1000] - Loss: 7.5207\n",
      "Epoch [972/1000] - Loss: 7.2679\n",
      "Epoch [973/1000] - Loss: 7.4215\n",
      "Epoch [974/1000] - Loss: 7.4121\n",
      "Epoch [975/1000] - Loss: 7.4556\n",
      "Epoch [976/1000] - Loss: 7.4063\n",
      "Epoch [977/1000] - Loss: 7.4389\n",
      "Epoch [978/1000] - Loss: 7.2305\n",
      "Epoch [979/1000] - Loss: 7.1098\n",
      "Epoch [980/1000] - Loss: 7.2944\n",
      "Epoch [981/1000] - Loss: 7.5300\n",
      "Epoch [982/1000] - Loss: 7.3848\n",
      "Epoch [983/1000] - Loss: 7.2750\n",
      "Epoch [984/1000] - Loss: 7.1916\n",
      "Epoch [985/1000] - Loss: 7.4703\n",
      "Epoch [986/1000] - Loss: 7.5761\n",
      "Epoch [987/1000] - Loss: 7.2992\n",
      "Epoch [988/1000] - Loss: 7.1941\n",
      "Epoch [989/1000] - Loss: 7.2793\n",
      "Epoch [990/1000] - Loss: 7.3975\n",
      "Epoch [991/1000] - Loss: 7.4477\n",
      "Epoch [992/1000] - Loss: 7.4651\n",
      "Epoch [993/1000] - Loss: 7.3473\n",
      "Epoch [994/1000] - Loss: 7.4134\n",
      "Epoch [995/1000] - Loss: 7.3516\n",
      "Epoch [996/1000] - Loss: 7.1988\n",
      "Epoch [997/1000] - Loss: 7.1624\n",
      "Epoch [998/1000] - Loss: 7.3878\n",
      "Epoch [999/1000] - Loss: 7.6579\n",
      "Epoch [1000/1000] - Loss: 7.5031\n",
      "Training time: 1494.56 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "loss_history = []\n",
    "\n",
    "num_batches = 100  # debug limit (increase later)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for train_epoch in range(num_batches):\n",
    "        input_batch, label_batch = random_batch(\n",
    "            batch_size,\n",
    "            tokenized_corpus,\n",
    "            word2index,\n",
    "            window_size\n",
    "        )\n",
    "\n",
    "        input_tensor = torch.LongTensor(input_batch).unsqueeze(1)\n",
    "        label_tensor = torch.LongTensor(label_batch).unsqueeze(1)\n",
    "\n",
    "        all_vocabs = prepare_all_vocabs(vocabs, word2index, batch_size)\n",
    "\n",
    "        loss = model(input_tensor, label_tensor, all_vocabs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Step 1: build final embedding matrix\n",
    "W = (model.embedding_center.weight.detach().cpu().numpy() +\n",
    "     model.embedding_outside.weight.detach().cpu().numpy()) / 2\n",
    "\n",
    "# Normalize embeddings for fast cosine similarity\n",
    "W_norm = W / np.linalg.norm(W, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "def get_vector(word):\n",
    "    if word not in word2index:\n",
    "        return None\n",
    "    return W_norm[word2index[word]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {idx: word for word, idx in word2index.items()}\n",
    "def predict_analogy(a, b, c, W_norm, word2index, index2word):\n",
    "    if a not in word2index or b not in word2index or c not in word2index:\n",
    "        return None\n",
    "\n",
    "    va = W_norm[word2index[a]]\n",
    "    vb = W_norm[word2index[b]]\n",
    "    vc = W_norm[word2index[c]]\n",
    "\n",
    "    # Vector arithmetic: b - a + c\n",
    "    target = vb - va + vc\n",
    "    target = target / np.linalg.norm(target)\n",
    "\n",
    "    # Cosine similarity with ALL words at once\n",
    "    similarities = np.dot(W_norm, target)\n",
    "\n",
    "    # Exclude input words\n",
    "    for w in (a, b, c):\n",
    "        similarities[word2index[w]] = -1\n",
    "\n",
    "    best_index = np.argmax(similarities)\n",
    "    return index2word[best_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_analogies(file_path, W_norm, word2index, index2word):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            if len(words) != 4:\n",
    "                continue\n",
    "\n",
    "            a, b, c, d = words\n",
    "            prediction = predict_analogy(\n",
    "                a, b, c, W_norm, word2index, index2word\n",
    "            )\n",
    "\n",
    "            if prediction is None:\n",
    "                continue\n",
    "\n",
    "            total += 1\n",
    "            if prediction == d:\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, correct, total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic and Syntactic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_acc, sem_correct, sem_total = evaluate_analogies(\n",
    "    \"country-capital.txt\",\n",
    "    W,\n",
    "    word2index,\n",
    "    index2word\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic accuracy (capital-common-countries): 0.0000 (0/144)\n",
      "Syntactic accuracy (past-tense): 0.0000 (0/648)\n"
     ]
    }
   ],
   "source": [
    "syntactic_acc, syn_correct, syn_total = evaluate_analogies(\n",
    "    \"past-tense.txt\",\n",
    "    W,\n",
    "    word2index,\n",
    "    index2word\n",
    ")\n",
    "\n",
    "print(f\"Semantic accuracy (capital-common-countries): {semantic_acc:.4f} ({sem_correct}/{sem_total})\")\n",
    "print(f\"Syntactic accuracy (past-tense): {syntactic_acc:.4f} ({syn_correct}/{syn_total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word 1    Word 2  Human (mean)\n",
       "0      love       sex          6.77\n",
       "1     tiger       cat          7.35\n",
       "2     tiger     tiger         10.00\n",
       "3      book     paper          7.46\n",
       "4  computer  keyboard          7.62"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "similarity_df = pd.read_csv(\"combined.csv\")\n",
    "similarity_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used pairs: 191\n",
      "Skipped OOV pairs: 162\n"
     ]
    }
   ],
   "source": [
    "model_scores = []\n",
    "human_scores = []\n",
    "skipped = 0\n",
    "\n",
    "for _, row in similarity_df.iterrows():\n",
    "    w1 = row[\"Word 1\"]\n",
    "    w2 = row[\"Word 2\"]\n",
    "    human_score = row[\"Human (mean)\"]\n",
    "\n",
    "    if w1 not in word2index or w2 not in word2index:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    v1 = W_norm[word2index[w1]]\n",
    "    v2 = W_norm[word2index[w2]]\n",
    "\n",
    "    model_sim = np.dot(v1, v2)  # cosine similarity\n",
    "\n",
    "    model_scores.append(model_sim)\n",
    "    human_scores.append(human_score)\n",
    "\n",
    "print(f\"Used pairs: {len(model_scores)}\")\n",
    "print(f\"Skipped OOV pairs: {skipped}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman Similarity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: -0.0602\n",
      "P-value: 4.0807e-01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "correlation, p_value = spearmanr(model_scores, human_scores)\n",
    "\n",
    "print(f\"Spearman correlation: {correlation:.4f}\")\n",
    "print(f\"P-value: {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " comparisons:\n",
      "love sex Human: 6.77 Model: 0.943\n",
      "tiger cat Human: 7.46 Model: 0.893\n",
      "tiger tiger Human: 5.77 Model: 0.919\n",
      "book paper Human: 6.31 Model: 0.99\n",
      "computer keyboard Human: 7.5 Model: 0.943\n"
     ]
    }
   ],
   "source": [
    "print(\" comparisons:\")\n",
    "for i in range(5):\n",
    "    print(\n",
    "        similarity_df.iloc[i, 0],\n",
    "        similarity_df.iloc[i, 1],\n",
    "        \"Human:\", human_scores[i],\n",
    "        \"Model:\", round(model_scores[i], 3)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Window Size</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Syntactic Accuracy</th>\n",
       "      <th>Semantic accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skipgram</td>\n",
       "      <td>2</td>\n",
       "      <td>7.503079</td>\n",
       "      <td>1494.55755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Window Size  Training Loss  Training time  Syntactic Accuracy  \\\n",
       "0  Skipgram            2       7.503079     1494.55755                 0.0   \n",
       "\n",
       "   Semantic accuracy  \n",
       "0                0.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Skipgram\"],\n",
    "    \"Window Size\": [window_size],\n",
    "    \"Training Loss\": [avg_loss],\n",
    "    \"Training time\": [end_time - start_time],\n",
    "    \"Syntactic Accuracy\": [syntactic_acc],\n",
    "    \"Semantic accuracy\": [semantic_acc]\n",
    "}\n",
    "\n",
    "df_skipgram = pd.DataFrame(results)\n",
    "df_skipgram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
